"""
Observer Service - Pure Sensory Layer with 6-Stage Pipeline

Philosophy: Sense, dont think. Observe, dont judge.

6-Stage Pipeline:
1. Ingestion - Receive and validate
2. Deduplication - Check Redis cache
3. Loop Check - Circular reference protection
4. Enrichment - Add Neo4j dimensional context
5. Storage - Persist to TimescaleDB
6. Distribution - Publish to NATS

NO LLMs. NO reasoning. NO decision making.
Just pure passive observation.
"""

import os
import logging
import json
from typing import Dict, Any, Optional
from datetime import datetime

from fastapi import FastAPI, Request, Response
from fastapi.responses import JSONResponse
import uvicorn
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import Observer components
from observer.event_schema import create_event, EventSource, NormalizedFields
from observer.adapters.slack_adapter import (
    normalize_slack_message,
    normalize_slack_button_click,
    normalize_slack_app_mention
)
from observer.storage.redis_cache import RedisCache
from observer.metrics import ObserverMetrics
from observer.nats_publisher import NATSPublisher
from observer.timescale_writer_enhanced import EnhancedTimescaleWriter
import redis.asyncio as redis_async

# Import new Observer modules
from observer.circular_protection import CircularReferenceProtector
from observer.neo4j_reader import ObserverNeo4jReader, EventTypeQuery
from observer.circuit_breaker import CircuitBreaker, CircuitBreakerOpen

# Configure logging
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("observer")

# Initialize FastAPI app
app = FastAPI(
    title="Observer - BasalMind Sensory Layer (6-Stage Pipeline)",
    description="Pure passive observation without judgment or reasoning",
    version="2.0.0"
)

# Global state (initialized on startup)
redis_cache: RedisCache = None
redis_async_client: Optional[redis_async.Redis] = None
metrics: ObserverMetrics = None
nats_publisher: Optional[NATSPublisher] = None
timescale_writer: Optional[EnhancedTimescaleWriter] = None
neo4j_reader: Optional[ObserverNeo4jReader] = None
neo4j_circuit_breaker: Optional[CircuitBreaker] = None


@app.on_event("startup")
async def startup_event():
    """Initialize Observer components on startup."""
    global redis_cache, redis_async_client, metrics, nats_publisher, timescale_writer, neo4j_reader, neo4j_circuit_breaker

    logger.info("üöÄ Starting Observer service (6-stage pipeline)...")

    # Initialize Redis cache (synchronous for dedup)
    redis_cache = RedisCache(
        host=os.getenv("REDIS_HOST", "localhost"),
        port=int(os.getenv("REDIS_PORT", 6390)),
        password=os.getenv("REDIS_PASSWORD"),
        db=int(os.getenv("REDIS_DB", 0))
    )

    # Initialize async Redis client (for WAL)
    redis_async_client = redis_async.Redis(
        host=os.getenv("REDIS_HOST", "localhost"),
        port=int(os.getenv("REDIS_PORT", 6390)),
        password=os.getenv("REDIS_PASSWORD"),
        db=int(os.getenv("REDIS_DB", 0)),
        decode_responses=False  # WAL needs binary mode for msgpack
    )

    # Initialize metrics
    metrics = ObserverMetrics()

    # Initialize Neo4j reader with circuit breaker (for enrichment stage)
    try:
        neo4j_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
        neo4j_user = os.getenv("NEO4J_USER", "neo4j")
        neo4j_password = os.getenv("NEO4J_PASSWORD")

        if neo4j_password:
            # Create circuit breaker for Neo4j
            neo4j_circuit_breaker = CircuitBreaker(
                name="neo4j",
                failure_threshold=5,
                recovery_timeout=60,
                expected_exception=Exception
            )

            neo4j_reader = ObserverNeo4jReader(neo4j_uri=neo4j_uri,
                neo4j_user=neo4j_user,
                neo4j_password=neo4j_password
            )
            logger.info(f"‚úÖ Neo4j reader connected: {neo4j_uri} (with circuit breaker)")
        else:
            logger.warning("‚ö†Ô∏è Neo4j credentials missing, skipping enrichment")
            neo4j_reader = None
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Neo4j connection failed (non-fatal): {e}")
        neo4j_reader = None

    # Initialize NATS publisher
    try:
        nats_url = os.getenv("NATS_URL", "nats://localhost:4222")
        stream_name = os.getenv("NATS_STREAM", "BASALMIND_EVENTS")

        nats_publisher = NATSPublisher(nats_url=nats_url, stream_name=stream_name)
        await nats_publisher.connect()
        logger.info(f"‚úÖ NATS connected: {nats_url}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è NATS connection failed (non-fatal): {e}")
        nats_publisher = None

    # Initialize Enhanced TimescaleDB writer (with batch processing, WAL, backpressure)
    try:
        host = os.getenv("TIMESCALE_HOST", "localhost")
        port = int(os.getenv("TIMESCALE_PORT", 5432))
        database = os.getenv("TIMESCALE_DB", "basalmind_events")
        user = os.getenv("TIMESCALE_USER", "basalmind")
        password = os.getenv("TIMESCALE_PASSWORD", "basalmind_secure_2024")
        batch_size = int(os.getenv("BATCH_SIZE", 100))
        flush_interval = float(os.getenv("FLUSH_INTERVAL", 1.0))

        timescale_writer = EnhancedTimescaleWriter(
            host=host,
            port=port,
            database=database,
            user=user,
            password=password,
            redis_client=redis_async_client,
            batch_size=batch_size,
            flush_interval=flush_interval
        )
        await timescale_writer.connect()
        logger.info(f"‚úÖ TimescaleDB connected: {host}:{port}/{database} (Enhanced with WAL & batching)")
    except Exception as e:
        logger.error(f"‚ùå TimescaleDB connection failed (CRITICAL): {e}")
        timescale_writer = None

    # Health check Redis
    health = redis_cache.health_check()
    if health.get("connected"):
        host = health.get("host", "unknown")
        port = health.get("port", "unknown")
        logger.info(f"‚úÖ Redis connected: {host}:{port}")
    else:
        error = health.get("error", "unknown error")
        logger.error(f"‚ùå Redis connection failed: {error}")

    logger.info("‚úÖ Observer service ready (6-stage pipeline active)")


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown."""
    global nats_publisher, neo4j_reader

    logger.info("üõë Shutting down Observer...")

    if nats_publisher:
        await nats_publisher.disconnect()

    if timescale_writer:
        await timescale_writer.disconnect()

    if neo4j_reader:
        neo4j_reader.close()

    logger.info("‚úÖ Observer shutdown complete")


async def process_event_pipeline(canonical_event: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process event through 6-stage pipeline.
    
    Stages:
    1. Ingestion - Already done (event normalized)
    2. Deduplication - Check Redis
    3. Loop Check - Circular reference protection
    4. Enrichment - Neo4j dimensional context
    5. Storage - TimescaleDB
    6. Distribution - NATS
    
    Returns: Processing result with status and metadata
    """
    
    # STAGE 2: DEDUPLICATION
    start_dedup = datetime.utcnow()
    event_data = {
        "source_system": canonical_event["source_system"],
        "event_type": canonical_event["event_type"],
        "user_id": canonical_event.get("normalized", {}).get("user_id"),
        "timestamp": canonical_event.get("normalized", {}).get("source_timestamp"),
        "text": canonical_event.get("normalized", {}).get("text", "")
    }
    
    is_duplicate = await redis_cache.is_duplicate(event_data)
    dedup_duration_ms = (datetime.utcnow() - start_dedup).total_seconds() * 1000
    
    if metrics:
        metrics.record_dedup_check(dedup_duration_ms, is_duplicate)
    
    if is_duplicate:
        logger.debug(f"[STAGE 2 - DEDUP] Duplicate event: {canonical_event.get('event_id')}")
        return {
            "status": "duplicate",
            "event_id": canonical_event.get("event_id"),
            "stage_reached": 2,
            "reason": "Duplicate detected in Redis cache"
        }
    
    # STAGE 3: LOOP CHECK (Circular Reference Protection)
    # Add metadata for loop detection
    event_with_metadata = {
        **canonical_event,
        "_depth": canonical_event.get("_depth", 0),
        "_originated_from": canonical_event["source_system"],
        "_processing_entity": "Observer"
    }
    
    should_process, rejection_reason = CircularReferenceProtector.should_process_event(
        event_with_metadata
    )
    
    if not should_process:
        logger.warning(f"[STAGE 3 - LOOP CHECK] Event rejected: {rejection_reason}")
        return {
            "status": "rejected",
            "event_id": canonical_event.get("event_id"),
            "stage_reached": 3,
            "reason": rejection_reason
        }
    
    logger.info(f"[STAGE 3 - LOOP CHECK] Event passed circular reference check")
    
    # STAGE 4: ENRICHMENT (Neo4j Dimensional Context with Circuit Breaker)
    enriched_event = canonical_event.copy()

    if neo4j_reader and neo4j_circuit_breaker:
        try:
            # Use circuit breaker to protect against Neo4j failures
            async with neo4j_circuit_breaker:
                # Query Neo4j for event type metadata
                event_type_name = canonical_event["event_type"]
                source_system = canonical_event["source_system"]

                # Check if event should be processed based on Neo4j filters
                should_process_neo4j, neo4j_reason = neo4j_reader.should_process_event(
                    event_type_name,
                    originated_from="Observer"
                )

                if not should_process_neo4j:
                    logger.warning(f"[STAGE 4 - ENRICHMENT] Neo4j filter rejected: {neo4j_reason}")
                    return {
                        "status": "rejected",
                        "event_id": canonical_event.get("event_id"),
                        "stage_reached": 4,
                        "reason": neo4j_reason
                    }

                # Get dimensional context (simplified for Observer)
                event_meta = neo4j_reader.get_event_type_metadata(event_type_name)

                if event_meta:
                    enriched_event["_enrichment"] = {
                        "priority": event_meta.get("priority"),
                        "category": event_meta.get("category"),
                        "retention_days": event_meta.get("retention_days"),
                        "consumed_by": event_meta.get("consumed_by", [])
                    }
                    logger.info(f"[STAGE 4 - ENRICHMENT] Added dimensional context: priority={event_meta.get('priority')}")
                else:
                    logger.debug(f"[STAGE 4 - ENRICHMENT] No metadata found for {event_type_name}")

        except CircuitBreakerOpen:
            # Circuit breaker is open - skip enrichment but continue processing
            logger.warning(f"[STAGE 4 - ENRICHMENT] Circuit breaker OPEN - skipping Neo4j enrichment")
            enriched_event["_enrichment"] = {"circuit_breaker": "open", "enriched": False}
                
        except Exception as e:
            logger.warning(f"[STAGE 4 - ENRICHMENT] Neo4j enrichment failed (non-fatal): {e}")
    else:
        logger.debug("[STAGE 4 - ENRICHMENT] Skipped (Neo4j not available)")
    
    # STAGE 5: STORAGE (TimescaleDB - Permanent)
    if timescale_writer and timescale_writer.is_connected:
        try:
            await timescale_writer.write_event(enriched_event)
            logger.info(f"[STAGE 5 - STORAGE] Event persisted to TimescaleDB")
        except Exception as e:
            logger.error(f"‚ùå CRITICAL [STAGE 5 - STORAGE]: TimescaleDB write failed: {e}")
            return {
                "status": "error",
                "event_id": canonical_event.get("event_id"),
                "stage_reached": 5,
                "reason": f"Storage failed: {str(e)}"
            }
    else:
        logger.error("‚ùå CRITICAL [STAGE 5 - STORAGE]: TimescaleDB not available, event NOT persisted!")
    
    # STAGE 6: DISTRIBUTION (NATS - Fire and Forget)
    if nats_publisher and nats_publisher.is_connected:
        try:
            # Prepare event for NATS (remove large payloads if needed)
            event_for_nats = {
                k: v for k, v in enriched_event.items()
                if k not in ["raw_payload"]  # Exclude raw payload to reduce message size
            }
            await nats_publisher.publish(event_for_nats)
            logger.info(f"[STAGE 6 - DISTRIBUTION] Event published to NATS")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [STAGE 6 - DISTRIBUTION] NATS publish failed (non-fatal): {e}")
    else:
        logger.debug("[STAGE 6 - DISTRIBUTION] Skipped (NATS not available)")
    
    # Pipeline complete
    return {
        "status": "observed",
        "event_id": canonical_event.get("event_id"),
        "event_type": canonical_event["event_type"],
        "stage_reached": 6,
        "dedup_ms": round(dedup_duration_ms, 2),
        "enriched": "_enrichment" in enriched_event
    }


@app.get("/")
async def root():
    """Root endpoint - service info."""
    return {
        "service": "Observer",
        "version": "2.0.0",
        "philosophy": "Sense, dont think. Observe, dont judge.",
        "pipeline_stages": 6,
        "status": "observing",
        "stages": {
            "1": "Ingestion (normalize)",
            "2": "Deduplication (Redis)",
            "3": "Loop Check (circular protection)",
            "4": "Enrichment (Neo4j dimensions)",
            "5": "Storage (TimescaleDB permanent)",
            "6": "Distribution (NATS pub/sub)"
        },
        "endpoints": {
            "health": "/health",
            "metrics": "/metrics",
            "slack_events": "/observe/slack/events",
            "slack_interactions": "/observe/slack/interactions",
            "internal_event": "/observe/internal/event"
        }
    }


@app.get("/health")
async def health():
    """Health check endpoint."""
    redis_health = redis_cache.health_check() if redis_cache else {"connected": False}
    neo4j_health = {"connected": neo4j_reader is not None}
    
    return {
        "status": "healthy" if redis_health.get("connected") else "degraded",
        "timestamp": datetime.utcnow().isoformat(),
        "components": {
            "redis": redis_health,
            "timescaledb": {"connected": timescale_writer.is_connected if timescale_writer else False},
            "neo4j": neo4j_health,
            "nats": {"connected": nats_publisher.is_connected if nats_publisher else False},
            "metrics": {
                "events_observed": metrics.events_observed if metrics else 0
            }
        }
    }



@app.post("/observe/cloudflare/event")
async def observe_cloudflare_event(request: Request):
    """
    Observe Cloudflare edge event.
    This endpoint receives real-time HTTP request events from Cloudflare Workers.
    """
    try:
        event_data = await request.json()
        
        # Validate required fields
        if "event_type" not in event_data:
            return JSONResponse(
                status_code=400,
                content={"status": "error", "message": "Missing required field: event_type"}
            )
        
        # Ensure required structure
        if "source_system" not in event_data:
            event_data["source_system"] = "cloudflare"
        
        # Add event_id if not present
        if "event_id" not in event_data:
            import uuid
            event_data["event_id"] = str(uuid.uuid4())
        
        # Add observed timestamp if not present
        if "observed_at" not in event_data:
            event_data["observed_at"] = datetime.utcnow().isoformat()
        
        # Process through pipeline
        result = await process_event_pipeline(event_data)
        return result
        
    except Exception as e:
        logger.error(f"Error processing Cloudflare event: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )



@app.get("/metrics")
async def get_metrics():
    """Get Observer metrics."""
    if not metrics:
        return {"error": "Metrics not initialized"}
    
    return metrics.get_summary()


@app.post("/observe/slack/events")
async def observe_slack_event(request: Request):
    """
    Observe Slack event webhook.
    
    Processes event through 6-stage pipeline.
    """
    try:
        raw_data = await request.json()
        
        # Slack URL verification challenge
        if raw_data.get("type") == "url_verification":
            logger.info("[SLACK] URL verification challenge received")
            return {"challenge": raw_data.get("challenge")}
        
        # Extract event
        slack_event = raw_data.get("event", {})
        event_type = slack_event.get("type")
        
        logger.info(f"[STAGE 1 - INGESTION] Slack event: {event_type}")
        
        # Record observation
        if metrics:
            metrics.record_event_observed("slack")
        
        # STAGE 1: INGESTION - Normalize to canonical format
        if event_type == "message":
            # Skip bot messages
            if slack_event.get("subtype") == "bot_message":
                logger.debug("[STAGE 1 - INGESTION] Skipping bot message")
                return {"status": "ignored", "reason": "bot_message"}
            
            canonical_event = normalize_slack_message(slack_event)
            
        elif event_type == "app_mention":
            canonical_event = normalize_slack_app_mention(slack_event)
            
        else:
            # Unknown type - create generic canonical event
            logger.warning(f"[STAGE 1 - INGESTION] Unknown Slack event type: {event_type}")
            canonical_event = create_event(
                event_type=f"slack.{event_type}",
                source_system=EventSource.SLACK,
                raw_payload=slack_event,
                normalized=NormalizedFields()
            )
        
        # Convert to dict for pipeline processing
        canonical_dict = {
            "event_id": canonical_event.event_id,
            "event_time": canonical_event.event_time.isoformat(),
            "observed_at": canonical_event.observed_at.isoformat(),
            "event_type": canonical_event.event_type,
            "source_system": canonical_event.source_system.value,
            "normalized": {
                k: v for k, v in canonical_event.normalized.__dict__.items()
                if v is not None
            },
            "session_id": canonical_event.session_id,
            "trace_id": canonical_event.trace_id,
            "correlation_id": canonical_event.correlation_id,
            "raw_payload": canonical_event.raw_payload
        }
        
        # Process through pipeline
        result = await process_event_pipeline(canonical_dict)
        
        return result
        
    except Exception as e:
        logger.error(f"[ERROR] Failed to observe event: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )


@app.post("/observe/internal/event")
async def observe_internal_event(request: Request):
    """
    Observe internal BasalMind event.
    
    This endpoint accepts events from other BasalMind components.
    """
    try:
        event_data = await request.json()
        
        logger.info(f"[STAGE 1 - INGESTION] Internal event: {event_data.get('event_type')}")
        
        if metrics:
            metrics.record_event_observed("internal")
        
        # Validate required fields
        if "event_type" not in event_data:
            return JSONResponse(
                status_code=400,
                content={"status": "error", "message": "Missing required field: event_type"}
            )
        
        # Ensure it has required structure
        if "source_system" not in event_data:
            event_data["source_system"] = "internal"
        
        if "event_id" not in event_data:
            import uuid
            event_data["event_id"] = str(uuid.uuid4())
        
        if "observed_at" not in event_data:
            event_data["observed_at"] = datetime.utcnow().isoformat()
        
        # Process through pipeline
        result = await process_event_pipeline(event_data)
        
        return result
        
    except Exception as e:
        logger.error(f"[ERROR] Failed to observe internal event: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )


def main():
    """Run the Observer service."""
    host = os.getenv("OBSERVER_HOST", "0.0.0.0")
    port = int(os.getenv("OBSERVER_PORT", 5001))
    
    logger.info(f"üëÅÔ∏è  Starting Observer on {host}:{port} (6-stage pipeline)")
    
    uvicorn.run(
        "observer.main_enhanced:app",
        host=host,
        port=port,
        log_level=os.getenv("LOG_LEVEL", "info").lower(),
        reload=False
    )


if __name__ == "__main__":
    main()
